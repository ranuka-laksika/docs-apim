# 🧱 Key Safety Categories Covered by WSO2 AI Guardrails

[![AI Gateway]({{base_path}}/assets/img/learn/ai-gateway/ai-guardrail-safety-categories.png){: style="width:40%"}]({{base_path}}/assets/img/learn/ai-gateway/ai-guardrail-safety-categories.png)

WSO2 AI Guardrails comprehensively address critical safety aspects across four foundational categories, ensuring secure, compliant, and reliable AI interactions:

| Category                  | Description                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------|
| 🔐 **LLM Safety** *(Documentation Coming Soon)*           | Protects against prompt injection attacks and enforces proper prompt structures to maintain model integrity. |
| ☣️ [**Content Safety**](../safety-categories/content-safety.md)       | Detects and filters toxic, harmful, or offensive content to ensure safe and appropriate AI outputs.             |
| 🎛 [**Content Usage Control**](../safety-categories/content-usage-control.md) | Implements organizational policies by enforcing word, sentences, and content usage guidelines consistently.    |
| 🕵️ **PII Safety** *(Documentation Coming Soon)*           | Identifies and redacts personally identifiable information (PII) to uphold privacy and regulatory compliance.    |


Explore the comprehensive guardrail capabilities offered within each safety category.
